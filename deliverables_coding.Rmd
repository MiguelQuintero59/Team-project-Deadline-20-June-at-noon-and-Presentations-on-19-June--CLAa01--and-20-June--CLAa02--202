---
title: "Group Project"
output: pdf_document
date: "2025-06-03"
---

# FUNCTIONS
```{r setup, include=FALSE}

library(MASS)
library(fmsb)
library(pROC)
library(dplyr)
library(caret)
library(scales) 
library(tidyr)
library(glmnet)
library(ggplot2)
library(reshape2)
library(pheatmap)
library(randomForest)

load_gene_subset <- function(group_number,
                             expression_file = "gene-expression-invasive-vs-noninvasive-cancer.csv",
                             subset_file = "teamsubsets.csv",
                             include_class = TRUE) {

  # Load main dataset
  dataframe <- read.csv(file = expression_file)

  # Load subset file
  df_subsets <- read.csv(file = subset_file, sep = ' ')

  # Filter by group number
  reg_id <- df_subsets[group_number, ]

  # Extract gene indices (remove the ID column)
  subsets <- as.numeric(reg_id[-1])

  # Filter gene expression dataframe using selected gene indices
  df <- dataframe[, subsets]

  # Optionally, add class and cancer type
  if (include_class) {
    df$Class <- dataframe$Class
  }

  return(df)
}

# Initialize empty results data frame
results_table <- data.frame(
  Model = character(),
  Hyperparameter = character(),
  Precision = numeric(),
  Sensitivity = numeric(),
  Specificity = numeric(),
  stringsAsFactors = FALSE
)

# Define the function
add_results <- function(model, hyperparameter, featureselection, precision, sensitivity, specificity) {
  new_row <- data.frame(
    Model = model,
    Hyperparameter = hyperparameter,
    FeatureSelection = featureselection,
    Precision = precision,
    Sensitivity = sensitivity,
    Specificity = specificity,
    stringsAsFactors = FALSE
  )
  
  # Use global assignment to update results_table
  assign("results_table", rbind(results_table, new_row), envir = .GlobalEnv)
}

#Read training and testing dataset
read_gene_data <- function(train_path = "data/train_data.csv", test_path = "data/test_data.csv") {
  # Read the CSV files
  train_df <- read.csv(train_path)
  test_df <- read.csv(test_path)
  
  # Split into genes and labels
  train_genes <- train_df[, !(names(train_df) %in% "Class")]
  train_labels <- train_df$Class
  
  test_genes <- test_df[, !(names(test_df) %in% "Class")]
  test_labels <- test_df$Class
  
  # Return as a list
  return(list(
    train_genes = train_genes,
    train_labels = train_labels,
    test_genes = test_genes,
    test_labels = test_labels
  ))
}


remove_correlated_features <- function(train_df, test_df = NULL, cutoff = 0.8) {
  # Compute correlation matrix on training data only
  cor_mat   <- cor(train_df, use = "pairwise.complete.obs")
  drop_idx  <- findCorrelation(cor_mat, cutoff = cutoff)
  keep_vars <- colnames(train_df)[-drop_idx]
  
  train_reduced <- train_df[, keep_vars, drop = FALSE]
  
  if (!is.null(test_df)) {
    test_reduced <- test_df[, keep_vars, drop = FALSE]
    return(list(train = train_reduced, test = test_reduced))
  } else {
    return(train_reduced)
  }
}

```


# The goal of feature selection is to highligth those features that have contribution performance towards the prediction variable. 
# LDA Feature Selection
```{r setup, include=FALSE}

set.seed(125) 
# LDA Dimensionality Reduction and Model  -- VERSION 1 
# Flow chart:LDA feature selection.jpeg

# The approach using LDA for dimensionality reduction is: 
# 1. Identify contribution performance using LDA to get features weights
# 2. Standardized Data
# 3. Random Search for Best Gene Subset
# 4. Prepare training and test sets with selected genes 
# 5. Extract metrics

# Load Data
df <- load_gene_subset(group_number = 7, include_class = TRUE)

# Separate predictors and class
df_features <- df[, setdiff(names(df), "Class")]
df_class <- df$Class

# Standardize predictors
df_features_scaled <- scale(df_features)

# Combine Dataframe
df_scaled <- as.data.frame(df_features_scaled)
df_scaled$Class <- make.names(df_class)


# Separate predictors and class
genes_only <- df_scaled[, setdiff(names(df_scaled), "Class")]
labels <- df_scaled$Class
all_genes <- colnames(genes_only)

# Create stratified train/test split
train_index <- createDataPartition(labels, p = 0.8, list = FALSE)
train_genes <- genes_only[train_index, ]
test_genes <- genes_only[-train_index, ]
train_labels <- labels[train_index]
test_labels <- labels[-train_index]

# ---------- Save Training and Test Data ----------
# Combine gene expression and class label
train_df <- data.frame(train_genes, Class = train_labels)
test_df <- data.frame(test_genes, Class = test_labels)

# Save to CSV files
write.csv(train_df, "data/train_data.csv", row.names = FALSE)
write.csv(test_df, "data/test_data.csv", row.names = FALSE)
# -----------------------------------------------

# ---------- Reading Training and Test Data ----------
data <- read_gene_data()
train_genes <- data$train_genes
train_labels <- data$train_labels
test_genes <- data$test_genes
test_labels <- data$test_labels
# ----------------------------------------------------


# === Random Search for Best Gene Subset ===
n_iter <- 30  # number of random samples
gene_range <- 10:60  # range of gene subset sizes to try
results <- data.frame()
weights_results <- data.frame()


for (i in 1:n_iter) {
  num_genes <- sample(gene_range, 1)
  selected_genes <- sample(all_genes, num_genes)
  
  # Prepare training and test sets with selected genes
  train_df <- data.frame(train_genes[, selected_genes], Class = train_labels)
  test_df <- data.frame(test_genes, Class = test_labels)
  
  
  # Fit LDA model
  lda_model <- tryCatch({
    lda(Class ~ ., data = train_df)
  }, error = function(e) return(NULL))
  
  feature_weights <- lda_model$scaling[selected_genes, , drop = FALSE]
  Weights = paste(paste(rownames(feature_weights), round(feature_weights[, 1], 5), sep = ":"), collapse = ";")

  
  if (!is.null(lda_model)) {
    # Predict on test set
    test_pred <- predict(lda_model, newdata = test_df)
    cm_test <- confusionMatrix(as.factor(test_pred$class), as.factor(test_df$Class), positive = "X1")

    # Predict again on full test_genes for AUC calculation
    test_pred_for_auc <- predict(lda_model, newdata = test_genes[, selected_genes])
    
    roc_lda_test <- pROC::roc(response = as.factor(test_labels),
                   predictor = test_pred_for_auc$posterior[, "X1"],
                   levels = rev(levels(as.factor(test_labels))))
    auc_lda_test <- auc(roc_lda_test)
  
    # Predict on training set
    train_pred <- predict(lda_model, newdata = train_df)
    cm_train <- confusionMatrix(as.factor(train_pred$class), as.factor(train_df$Class), positive = "X1")
    
    roc_lda_train <- pROC::roc(response = as.factor(train_df$Class),
                           predictor = train_pred$posterior[, "X1"],
                           levels = rev(levels(as.factor(train_df$Class))))
    auc_lda_train <- auc(roc_lda_train)
  
    # Append to results
    weights_results <- rbind(weights_results, data.frame(Weights = paste(paste(rownames(feature_weights), round(feature_weights[, 1], 5), sep = ":"), collapse = ";")))
    
    results <- rbind(results, data.frame(
      Iteration = i,
      NumGenes = num_genes,
      FeatureSelection = TRUE,
      
      AccuracyTrain = as.numeric(cm_train$overall["Accuracy"]), 
      Precision_Train = as.numeric(cm_train$byClass["Pos Pred Value"]),
      Sensitivity_Train = as.numeric(cm_train$byClass["Sensitivity"]),
      Specificity_Train = as.numeric(cm_train$byClass["Specificity"]),
      PPV_Train = as.numeric(cm_train$byClass["Pos Pred Value"]),
      NPV_Train = as.numeric(cm_train$byClass["Neg Pred Value"]),
      AUC_Train = as.numeric(auc_lda_train),
      
      Accuracy_Test = as.numeric(cm_test$overall["Accuracy"]),
      Precision_Test = as.numeric(cm_test$byClass["Pos Pred Value"]),
      Sensitivity_Test = as.numeric(cm_test$byClass["Sensitivity"]),
      Specificity_Test = as.numeric(cm_test$byClass["Specificity"]),
      PPV = as.numeric(cm_test$byClass["Pos Pred Value"]),
      NPV = as.numeric(cm_test$byClass["Neg Pred Value"]),
      AUC_Test = as.numeric(auc_lda_test),
      
      Genes = paste(selected_genes, collapse = ",")

    ))
  }
}

# Analyzing the output from the random search
# Getting rid of those models with higher overfitting
best_results <- results %>%
  filter(
    Precision_Train != 1,
    Sensitivity_Train != 1,
    Specificity_Train != 1,
    Precision_Test != 1,
    Sensitivity_Test != 1,
    Specificity_Test != 1
  ) %>%
  #Ordering by precision test results
  arrange(desc(Precision_Test)) %>%
  
  #Filter by max precision and max sensitivity to control the trade off by the confusion matrix outputs
  mutate(
    max_precision = max(Precision_Test, na.rm = TRUE),
    max_sensitivity = max(Sensitivity_Test, na.rm = TRUE)
  ) %>%
  
  filter(
    Precision_Test == max_precision |
    Sensitivity_Test == max_sensitivity
  ) %>%
  
  dplyr::select(-max_precision, -max_sensitivity)


# Ensure AUC_Train and AUC_Test exist (add NA if missing)
if (!"AUC_Train" %in% names(best_results)) best_results$AUC_Train <- NA
if (!"AUC_Test" %in% names(best_results)) best_results$AUC_Test <- NA

# Filter only the first row
best_single <- best_results %>%
  slice(1) %>%
  transmute(
    Model = "LDA",
    nfeatures = NumGenes,
    Genes,
    
    Accuracy_Train = AccuracyTrain,
    Accuracy_Test = Accuracy_Test,
    
    Sensitivity_Train = Sensitivity_Train,
    Sensitivity_Test = Sensitivity_Test,
    
    Specificity_Train = Specificity_Train,
    Specificity_Test = Specificity_Test,
    
    PPV_Train = PPV_Train,
    PPV_Test = PPV,
    
    NPV_Train = NPV_Train,
    NPV_Test = NPV,
    
    AUC_Train = AUC_Train,
    AUC_Test = AUC_Test
  ) %>%
  pivot_longer(
    cols = -c(Model, nfeatures, Genes),
    names_to = c("Metric", "Subset"),
    names_sep = "_"
  ) %>%
  pivot_wider(
    names_from = Metric,
    values_from = value
  ) %>%
  relocate(Model, Subset, Accuracy, Sensitivity, Specificity, PPV, NPV, AUC, nfeatures, Genes)

lda_metrics <- best_single %>%
  rename(Nfeats = nfeatures) %>%
  select(-Genes)

```


# Model:Random Forest Full Variables
# Resample Techinque: K - Fold Cross Validation
```{r setup, include=FALSE}
#The following code block is performing a Random forest (RF) using as resampling technique 10 fold cross validation. 
set.seed(125) 

# Load Data
df <- load_gene_subset(group_number = 7, include_class = TRUE)

# Separate predictors and class
genes_only <- df[, setdiff(names(df), "Class")]
all_genes <- colnames(genes_only)

# Create training 80% and test 20%, 
train <- createDataPartition(df$Class, p = 0.8, list = FALSE, times = 1)
df_train <- df[train,]
df_test <- df[-train,]


# Define hyperparameter grid for RF
num_features <- ncol(df_train) - 1
sqrt_mtry    <- floor(sqrt(num_features))
tunegrid_rf  <- expand.grid(
                  mtry = c(max(1, sqrt_mtry - 2), sqrt_mtry, sqrt_mtry + 2)
                )

# TrainControl must include classProbs = TRUE
ctrl <- trainControl(
  method = "cv",
  search = "grid",
  number = 10,
  classProbs = TRUE,
  savePredictions = "final",
  summaryFunction = twoClassSummary
)

# Target variable is a factor
df_train$Class         <- as.factor(df_train$Class)
levels(df_train$Class) <- make.names(levels(df_train$Class))

# Train random forest model with tuneGrid and cross-validation
model_rf <- train(
  Class ~ .,
  data = df_train,
  method = "rf",
  trControl = ctrl,
  preProcess = c("center", "scale"),
  tuneGrid = expand.grid(mtry = floor(sqrt(ncol(df_train) - 1))),
  ntree = 100,
  metric = "ROC"
)

# === Training Model Stats === 
roc_obj <- pROC::roc(
  response = model_rf$pred$obs,
  predictor = model_rf$pred$X1,  # X1 is the class label "1" after make.names()
  levels = rev(levels(model_rf$pred$obs))
)

auc_value_rf <- auc(roc_obj)

# Get predicted classes and true labels
predictions <- model_rf$pred$pred
true_labels <- model_rf$pred$obs

# Confusion Matrix
cm <- confusionMatrix(predictions, true_labels)

metrics_rf_train_cv <- data.frame(
  Model = "RF",
  resampling = 'CV',
  Subset = 'Train',
  Accuracy = as.numeric(cm$overall["Accuracy"]),
  Sensitivity = as.numeric(cm$byClass["Sensitivity"]),
  Specificity = as.numeric(cm$byClass["Specificity"]),
  PPV = as.numeric(cm$byClass["Pos Pred Value"]),
  NPV = as.numeric(cm$byClass["Neg Pred Value"]),
  AUC = as.numeric(auc_value_rf),
  Nfeats = as.numeric(ncol(df_train) - 1 )
)

# === Test Model Stats ===
prob_test <- predict(model_rf, newdata = df_test, type = "prob")
pred_class_test <- predict(model_rf, newdata = df_test, type = "raw")

df_test$Class <- make.names(df_test$Class)
df_test$Class <- factor(df_test$Class, levels = levels(pred_class_test))

cm_test <- confusionMatrix(
  pred_class_test,
  reference = as.factor(df_test$Class), 
  positive = "X1"
  )

# AUC
roc_test_rf <- pROC::roc(response = as.factor(df_test$Class),
               predictor = prob_test$X1,
               levels = rev(levels(as.factor(df_test$Class))))

auc_value_rf_test <- auc(roc_test_rf)

metrics_rf_test_cv <- data.frame(
  Model = "RF",
  resampling = 'CV',
  Subset = 'Test',
  Accuracy = as.numeric(cm_test$overall["Accuracy"]),
  Sensitivity = as.numeric(cm_test$byClass["Sensitivity"]),
  Specificity = as.numeric(cm_test$byClass["Specificity"]),
  PPV = as.numeric(cm_test$byClass["Pos Pred Value"]),
  NPV = as.numeric(cm_test$byClass["Neg Pred Value"]),
  AUC = as.numeric(auc_value_rf_test),
  Nfeats = ncol(df_test) - 1
)
```


# Model:Random Forest - Feature Selection
# Resample Techinque: K - Fold Cross Validation
```{r setup, include=FALSE}
# Reproducible
set.seed(125) 

# Load Data
df <- load_gene_subset(group_number = 7, include_class = TRUE)
selected_genes <- unlist(strsplit(best_results$Genes[1], split = ","))
selected_genes <- trimws(selected_genes)

#Make sure to filter contributing genes and class label in the df_filtered
df_filtered <- df[, selected_genes, drop = FALSE]
df_filtered <- df[, c(selected_genes, "Class")]

# Separate predictors and class
genes_only <- df_filtered[, setdiff(names(df_filtered), "Class")]
all_genes <- colnames(genes_only)

# Create training 80% and test 20%, 
train <- createDataPartition(df_filtered$Class, p = 0.8, list = FALSE, times = 1)
df_train <- df_filtered[train,]
df_test <- df_filtered[-train,]

# Define hyperparameter grid for RF
num_features <- ncol(df_train) - 1
sqrt_mtry    <- floor(sqrt(num_features))
tunegrid_rf  <- expand.grid(
                  mtry = c(max(1, sqrt_mtry - 2), sqrt_mtry, sqrt_mtry + 2)
                )

# TrainControl must include classProbs = TRUE
ctrl <- trainControl(
  method = "cv",
  search = "grid",
  number = 10,
  classProbs = TRUE,
  savePredictions = "final",
  summaryFunction = twoClassSummary
)

# Target variable is a factor
df_train$Class         <- as.factor(df_train$Class)
levels(df_train$Class) <- make.names(levels(df_train$Class))


# Train random forest model with tuneGrid and cross-validation
model_rf <- train(
  Class ~ .,
  data = df_train,
  method = "rf",
  trControl = ctrl,
  preProcess = c("center", "scale"),
  tuneGrid = expand.grid(mtry = floor(sqrt(ncol(df_train) - 1))),
  ntree = 100,
  metric = "ROC"
)


# === Training Model Stats === 
roc_obj <- pROC::roc(
  response = model_rf$pred$obs,
  predictor = model_rf$pred$X1,  # X1 is the class label "1" after make.names()
  levels = rev(levels(model_rf$pred$obs))
)

auc_value_rf <- auc(roc_obj)

# Get predicted classes and true labels
predictions <- model_rf$pred$pred
true_labels <- model_rf$pred$obs

# Confusion Matrix
cm <- confusionMatrix(predictions, true_labels)

metrics_rf_train_cv_fs <- data.frame(
  Model = "RF",
  resampling = 'CV',
  Subset = 'Train',
  Accuracy = as.numeric(cm$overall["Accuracy"]),
  Sensitivity = as.numeric(cm$byClass["Sensitivity"]),
  Specificity = as.numeric(cm$byClass["Specificity"]),
  PPV = as.numeric(cm$byClass["Pos Pred Value"]),
  NPV = as.numeric(cm$byClass["Neg Pred Value"]),
  AUC = as.numeric(auc_value_rf),
  Nfeats = as.numeric(ncol(df_train) - 1 )
)

# === Test Model Stats ===
prob_test <- predict(model_rf, newdata = df_test, type = "prob")
pred_class_test <- predict(model_rf, newdata = df_test, type = "raw")

df_test$Class <- make.names(df_test$Class)
df_test$Class <- factor(df_test$Class, levels = levels(pred_class_test))

cm_test <- confusionMatrix(
  pred_class_test,
  reference = as.factor(df_test$Class), 
  positive = "X1"
  )

# AUC
roc_test_rf <- pROC::roc(response = as.factor(df_test$Class),
               predictor = prob_test$X1,
               levels = rev(levels(as.factor(df_test$Class))))

auc_value_rf_test <- auc(roc_test_rf)

metrics_rf_test_cv_fs <- data.frame(
  Model = "RF",
  resampling = 'CV',
  Subset = 'Test',
  Accuracy = as.numeric(cm_test$overall["Accuracy"]),
  Sensitivity = as.numeric(cm_test$byClass["Sensitivity"]),
  Specificity = as.numeric(cm_test$byClass["Specificity"]),
  PPV = as.numeric(cm_test$byClass["Pos Pred Value"]),
  NPV = as.numeric(cm_test$byClass["Neg Pred Value"]),
  AUC = as.numeric(auc_value_rf_test),
  Nfeats = ncol(df_test) - 1
)

```



# Model:Random Forest Full Variables
# No Resample Technique
```{r setup, include=FALSE}

#The following code block is performing a Random forest (RF) using as resampling technique 10 fold cross validation. 
set.seed(125) 

# Load Data
df <- load_gene_subset(group_number = 7, include_class = TRUE)

# Separate predictors and class
genes_only <- df[, setdiff(names(df), "Class")]
all_genes <- colnames(genes_only)

# Create training 80% and test 20%, 
train <- createDataPartition(df$Class, p = 0.8, list = FALSE, times = 1)
df_train <- df[train,]
df_test <- df[-train,]

# Define hyperparameter RF
num_features <- ncol(df_train) - 1
sqrt_mtry    <- floor(sqrt(ncol(df_train) - 1))

ctrl <- trainControl(
  method = "none",
  number = 10,
  classProbs = TRUE,
  savePredictions = "final",
  summaryFunction = twoClassSummary
)

# Target variable is a factor
df_train$Class         <- as.factor(df_train$Class)
levels(df_train$Class) <- make.names(levels(df_train$Class))

# Train random forest model with tuneGrid and cross-validation
model_rf <- train(
  Class ~ .,
  data = df_train,
  method = "rf",
  trControl = ctrl,
  preProcess = c("center", "scale"),
  tuneGrid = expand.grid(mtry = sqrt_mtry),
  ntree = 100,
  metric = "ROC"
)

# === Training Model Stats === 
probs <- predict(model_rf, df_train, type = "prob")[, "X1"]
roc_obj <- pROC::roc(
  response = df_train$Class,
  predictor = probs,  # X1 is the class label "1" after make.names()
  levels = c("X2","X1")
)

auc_value_rf <- auc(roc_obj)

# Get predicted classes and true labels
predictions <- predict(model_rf, df_train)
true_labels <- df_train$Class

# Confusion Matrix
cm <- confusionMatrix(predictions, true_labels, )

metrics_rf_train <- data.frame(
  Model = "RF",
  resampling = 'No RS',
  Subset = 'Train',
  Accuracy = as.numeric(cm$overall["Accuracy"]),
  Sensitivity = as.numeric(cm$byClass["Sensitivity"]),
  Specificity = as.numeric(cm$byClass["Specificity"]),
  PPV = as.numeric(cm$byClass["Pos Pred Value"]),
  NPV = as.numeric(cm$byClass["Neg Pred Value"]),
  AUC = as.numeric(auc_value_rf),
  Nfeats = as.numeric(ncol(df_train) - 1 )
)


# === Test Model Stats ===
prob_test <- predict(model_rf, df_test, type = "prob")[, "X1"]
pred_class_test <- predict(model_rf, newdata = df_test, type = "raw")

df_test$Class <- make.names(df_test$Class)
df_test$Class <- factor(df_test$Class, levels = levels(pred_class_test))

# AUC
roc_test_rf <- pROC::roc(
  response = as.factor(df_test$Class),
  predictor = prob_test,
  levels = c("X2","X1")
  )

auc_value_rf_test <- auc(roc_test_rf)

# Get predicted classes and true labels
predictions <- predict(model_rf, df_test)
true_labels <- df_test$Class

cm_test <- confusionMatrix(
  predictions,
  true_labels, 
  positive = "X1"
  )


metrics_rf_test <- data.frame(
  Model = "RF",
  resampling = 'No RS',
  Subset = 'Test',
  Accuracy = as.numeric(cm_test$overall["Accuracy"]),
  Sensitivity = as.numeric(cm_test$byClass["Sensitivity"]),
  Specificity = as.numeric(cm_test$byClass["Specificity"]),
  PPV = as.numeric(cm_test$byClass["Pos Pred Value"]),
  NPV = as.numeric(cm_test$byClass["Neg Pred Value"]),
  AUC = as.numeric(auc_value_rf_test),
  Nfeats = ncol(df_test) - 1
)


metrics_rf <- rbind(
  metrics_rf_train_cv,
  metrics_rf_test_cv,
  metrics_rf_train_cv_fs,
  metrics_rf_test_cv_fs,
  metrics_rf_train,
  metrics_rf_test
)
```


#Perform SVM using resampling technique 
```{r setup, include=FALSE}

set.seed(125) 
# Load Data filter using feature selection from first point 
df <- load_gene_subset(group_number = 7, include_class = TRUE)
selected_genes <- unlist(strsplit(best_results$Genes[1], split = ","))
selected_genes <- trimws(selected_genes)

#Make sure to filter contributing genes and class label in the df_filtered
df_filtered <- df[, selected_genes, drop = FALSE]
df_filtered <- df[, c(selected_genes, "Class")]

# Separate predictors and class
genes_only <- df_filtered[, setdiff(names(df_filtered), "Class")]
labels <- df_filtered$Class
all_genes <- colnames(genes_only)

# Create stratified train/test split
train_index <- createDataPartition(labels, p = 0.8, list = FALSE)

train_genes <- genes_only[train_index, ]
test_genes <- genes_only[-train_index, ]
train_labels <- labels[train_index]
test_labels <- labels[-train_index]

#Combine dataframes
train_df <- data.frame(train_genes, Class = train_labels)
test_df <- data.frame(test_genes, Class = test_labels)

# Target variable is a factor
train_df$Class <- as.factor(train_df$Class)
levels(train_df$Class) <- make.names(levels(train_df$Class))

# TrainControl must include classProbs = TRUE
ctrl_svm <- trainControl(
  method = "cv",
  search = "grid",
  number = 10,
  classProbs = TRUE,
  savePredictions = "final",
  summaryFunction = twoClassSummary
)

# Train SVM model with cross-validation and considering feature selection from LDA 
model_sv <- train(
  Class ~ .,
  data = train_df,
  method = "svmRadial",
  trControl = ctrl_svm,
  preProcess = c("center", "scale"),
  ntree = 100,
  tuneLength = 10,
  metric = "ROC"
)

roc_obj <- pROC::roc(
  response = model_sv$pred$obs,
  predictor = model_sv$pred$X1,  # X1 is the class label "1" after make.names()
  levels = rev(levels(model_sv$pred$obs))
)

auc_value_svm <- auc(roc_obj)

# Get predicted classes and true labels
predictions <- model_sv$pred$pred
true_labels <- model_sv$pred$obs

# Confusion Matrix
cm <- confusionMatrix(predictions, true_labels)

metrics_svm_train <- data.frame(
  Model = "SVM",
  Subset = 'Train',
  Accuracy = as.numeric(cm$overall["Accuracy"]),
  Sensitivity = as.numeric(cm$byClass["Sensitivity"]),
  Specificity = as.numeric(cm$byClass["Specificity"]),
  PPV = as.numeric(cm$byClass["Pos Pred Value"]),
  NPV = as.numeric(cm$byClass["Neg Pred Value"]),
  AUC = as.numeric(auc_value_svm),
  Nfeats = as.numeric(ncol(train_df) - 1)
)


# === Test Model Stats ===
prob_test <- predict(model_sv, newdata = test_df, type = "prob")
pred_class_test <- predict(model_sv, newdata = test_df, type = "raw")
#pred_class_test <- factor(pred_class_test, levels = levels(test_df$Class))

test_df$Class <- make.names(test_df$Class)
test_df$Class <- factor(test_df$Class, levels = levels(pred_class_test))

cm_test <- confusionMatrix(
  pred_class_test,
  reference = test_df$Class, 
  positive = "X1"
  )

# AUC
roc_svm <- pROC::roc(response = as.factor(test_df$Class),
               predictor = prob_test$X1,
               levels = rev(levels(as.factor(test_df$Class))))
auc_value_svm <- auc(roc_svm)

metrics_svm_test <- data.frame(
  Model = "SVM",
  Subset = 'Test',
  Accuracy = as.numeric(cm_test$overall["Accuracy"]),
  Sensitivity = as.numeric(cm_test$byClass["Sensitivity"]),
  Specificity = as.numeric(cm_test$byClass["Specificity"]),
  PPV = as.numeric(cm_test$byClass["Pos Pred Value"]),
  NPV = as.numeric(cm_test$byClass["Neg Pred Value"]),
  AUC = as.numeric(auc_value_svm),
  Nfeats = as.numeric(ncol(test_df) - 1)
)


```


#QDA Model
```{r setup, include=FALSE}

set.seed(125) 
# Load Data filter using feature selection from first point 
df <- load_gene_subset(group_number = 7, include_class = TRUE)
selected_genes <- unlist(strsplit(best_results$Genes[1], split = ","))
selected_genes <- trimws(selected_genes)

#Make sure to filter contributing genes and class label in the df_filtered
df_filtered <- df[, selected_genes, drop = FALSE]
df_filtered <- df[, c(selected_genes, "Class")]

# Separate predictors and class
genes_only <- df_filtered[, setdiff(names(df_filtered), "Class")]
labels <- df_filtered$Class
all_genes <- colnames(genes_only)

# Create training 80% and test 20%, 
train <- createDataPartition(df_filtered$Class, p = 0.8, list = FALSE, times = 1)
df_train <- df_filtered[train,]
df_test <- df_filtered[-train,]

# Remove near-zero variance predictors
nzv <- nearZeroVar(df_train, saveMetrics = TRUE)
df_train <- df_train[, !nzv$nzv]

# Remove highly correlated predictors
df_train <- remove_correlated_features(df_train, threshold = 0.8)

# Convert Class to factor and apply make.names
df_train$Class <- as.factor(make.names(df_train$Class))
df_test$Class <- as.factor(make.names(df_test$Class))



cor_matrix <- cor(df_train[, sapply(df_train, is.numeric)])
high_corr <- findCorrelation(cor_matrix, cutoff = 0.9)
df_train <- df_train[, -high_corr]

# Train control setup
ctrl <- trainControl(
  method = "cv",
  number = 10,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = "final"
)

# Training QDA model
model_qda <- train(
  Class ~ .,
  data = df_train,
  method = "qda",
  trControl = ctrl,
  preProcess = c("center", "scale"),
  metric = "ROC"
)



# === Training Model Stats === 
roc_obj <- pROC::roc(
  response = model_qda$pred$obs,
  predictor = model_qda$pred$X1,  # X1 is the class label "1" after make.names()
  levels = rev(levels(model_qda$pred$obs))
)

auc_value_qda <- auc(roc_obj)

# Get predicted classes and true labels
predictions <- model_qda$pred$pred
true_labels <- model_qda$pred$obs

# Confusion Matrix
cm_qda <- confusionMatrix(predictions, true_labels)

metrics_qda_train <- data.frame(
  Model = "RF",
  Subset = 'Train',
  Accuracy = as.numeric(cm_qda$overall["Accuracy"]),
  Sensitivity = as.numeric(cm_qda$byClass["Sensitivity"]),
  Specificity = as.numeric(cm_qda$byClass["Specificity"]),
  PPV = as.numeric(cm_qda$byClass["Pos Pred Value"]),
  NPV = as.numeric(cm_qda$byClass["Neg Pred Value"]),
  AUC = as.numeric(auc_value_qda),
  Nfeats = as.numeric(ncol(df_train) - 1 )
)
```


# PLOTING RESULTS
```{r setup, include=FALSE}

# === Setting Up Radar chart plot ===

metrics_all <- rbind(lda_metrics,
                     metrics_rf_train, metrics_rf_test,
                     metrics_svm_train,metrics_svm_test)

metrics_all <- rbind(best_single,
                     metrics_rf_train,metrics_rf_test)

                     metrics_svm_train,metrics_svm_test)

metrics_all <- rbind(metrics_lda_test,
                     metrics_rf_train,metrics_rf_test,
                     metrics_svm_train,metrics_svm_test,
                     metrics_qda_train)

```
