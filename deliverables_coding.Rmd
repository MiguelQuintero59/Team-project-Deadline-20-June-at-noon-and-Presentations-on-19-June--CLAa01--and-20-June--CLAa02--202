---
title: "Group Project"
output: pdf_document
date: "2025-06-03"
---

# FUNCTIONS
```{r setup, include=FALSE}
library(MASS)
library(fmsb)
library(pROC)
library(dplyr)
library(caret)
library(glmnet)
library(ggplot2)
library(reshape2)
library(pheatmap)
library(randomForest)

load_gene_subset <- function(group_number,
                             expression_file = "gene-expression-invasive-vs-noninvasive-cancer.csv",
                             subset_file = "teamsubsets.csv",
                             include_class = TRUE) {

  # Load main dataset
  dataframe <- read.csv(file = expression_file)

  # Load subset file
  df_subsets <- read.csv(file = subset_file, sep = ' ')

  # Filter by group number
  reg_id <- df_subsets[group_number, ]

  # Extract gene indices (remove the ID column)
  subsets <- as.numeric(reg_id[-1])

  # Filter gene expression dataframe using selected gene indices
  df <- dataframe[, subsets]

  # Optionally, add class and cancer type
  if (include_class) {
    df$Class <- dataframe$Class
  }

  return(df)
}

# Initialize empty results data frame
results_table <- data.frame(
  Model = character(),
  Hyperparameter = character(),
  Precision = numeric(),
  Sensitivity = numeric(),
  Specificity = numeric(),
  stringsAsFactors = FALSE
)

# Define the function
add_results <- function(model, hyperparameter, featureselection, precision, sensitivity, specificity) {
  new_row <- data.frame(
    Model = model,
    Hyperparameter = hyperparameter,
    FeatureSelection = featureselection,
    Precision = precision,
    Sensitivity = sensitivity,
    Specificity = specificity,
    stringsAsFactors = FALSE
  )
  
  # Use global assignment to update results_table
  assign("results_table", rbind(results_table, new_row), envir = .GlobalEnv)
}

#Read training and testing dataset
read_gene_data <- function(train_path = "data/train_data.csv", test_path = "data/test_data.csv") {
  # Read the CSV files
  train_df <- read.csv(train_path)
  test_df <- read.csv(test_path)
  
  # Split into genes and labels
  train_genes <- train_df[, !(names(train_df) %in% "Class")]
  train_labels <- train_df$Class
  
  test_genes <- test_df[, !(names(test_df) %in% "Class")]
  test_labels <- test_df$Class
  
  # Return as a list
  return(list(
    train_genes = train_genes,
    train_labels = train_labels,
    test_genes = test_genes,
    test_labels = test_labels
  ))
}


remove_correlated_features <- function(train_data, threshold = 0.8) {
  # Compute correlation matrix
  cor_matrix <- cor(train_data, method = "pearson", use = "pairwise.complete.obs")
  cor_df <- as.data.frame(as.table(cor_matrix))
  
  # Remove self-correlations
  cor_df <- cor_df[cor_df$Var1 != cor_df$Var2, ]
  
  # Remove duplicate pairs (symmetry in correlation matrix)
  cor_df <- cor_df[!duplicated(t(apply(cor_df[, 1:2], 1, sort))), ]
  
  # Filter for highly correlated pairs
  high_cor_df <- cor_df[abs(cor_df$Freq) > threshold, ]
  
  # Keep only one variable from each correlated pair
  genes_to_remove <- unique(high_cor_df$Var2)
  
  # Remove redundant features
  df_reduced <- train_data[, !(colnames(train_data) %in% genes_to_remove)]
  
  return(df_reduced)
}

results <- data.frame(Iteration = integer(),
                      NumGenes = integer(),
                      FeatureSelection = logical(),
                      Precision = double(),
                      Sensitivity = double(),
                      Specificity = double(),
                      Genes = character(),
                      stringsAsFactors = FALSE)


```
# The goal of feature selection is to highligth those features that have contribution performance towards the prediction variable. 
# LDA Feature Selection
```{r setup, include=FALSE}

set.seed(125) 
# LDA Dimensionality Reduction and Model  -- VERSION 1
# The approach using LDA for dimensionality reduction is: 
# 1. Identify contribution performance using LDA to get features weights
# 2. Standardized Data
# 3. Random Search for Best Gene Subset
# 4. Prepare training and test sets with selected genes 
# 5. Extract metrics

# Load Data
df <- load_gene_subset(group_number = 7, include_class = TRUE)

# Extract Predictors
gene_data <- df[, setdiff(names(df), "Class")]

# Separate predictors and class
genes_only <- df[, setdiff(names(df), "Class")]
labels <- df$Class
all_genes <- colnames(genes_only)

# Create stratified train/test split
train_index <- createDataPartition(labels, p = 0.8, list = FALSE)
train_genes <- genes_only[train_index, ]
test_genes <- genes_only[-train_index, ]
train_labels <- labels[train_index]
test_labels <- labels[-train_index]

# ---------- Save Training and Test Data ----------
# Combine gene expression and class label
train_df <- data.frame(train_genes, Class = train_labels)
test_df <- data.frame(test_genes, Class = test_labels)

# Save to CSV files
write.csv(train_df, "data/train_data.csv", row.names = FALSE)
write.csv(test_df, "data/test_data.csv", row.names = FALSE)
# -----------------------------------------------

# ---------- Reading Training and Test Data ----------
data <- read_gene_data()
train_genes <- data$train_genes
train_labels <- data$train_labels
test_genes <- data$test_genes
test_labels <- data$test_labels
# ----------------------------------------------------

# Standardize training data
train_genes <- scale(train_genes)


# === Random Search for Best Gene Subset ===

n_iter <- 30  # number of random samples
gene_range <- 10:60  # range of gene subset sizes to try
results <- data.frame()

for (i in 1:n_iter) {
  num_genes <- sample(gene_range, 1)
  selected_genes <- sample(all_genes, num_genes)
  
  # Prepare training and test sets with selected genes
  train_df <- data.frame(train_genes[, selected_genes], Class = train_labels)
  test_df <- data.frame(test_genes, Class = test_labels)
  
  # Compute correlation matrix
  train_df <- remove_correlated_features(train_df)
  
  # Fit LDA model
  lda_model <- tryCatch({
    lda(Class ~ ., data = train_df)
  }, error = function(e) return(NULL))
  
  if (!is.null(lda_model)) {
    # Predict on test set
    test_pred <- predict(lda_model, newdata = test_df)
    cm_test <- confusionMatrix(as.factor(test_pred$class), as.factor(test_df$Class), positive = "1")
    
    # Predict on training set
    train_pred <- predict(lda_model, newdata = train_df)
    cm_train <- confusionMatrix(as.factor(train_pred$class), as.factor(train_df$Class), positive = "1")
    
    # Extract metrics
    precision_test <- cm_test$byClass["Pos Pred Value"]
    sensitivity_test <- cm_test$byClass["Sensitivity"]
    specificity_test <- cm_test$byClass["Specificity"]
    
    precision_train <- cm_train$byClass["Pos Pred Value"]
    sensitivity_train <- cm_train$byClass["Sensitivity"]
    specificity_train <- cm_train$byClass["Specificity"]
    
    # Append to results
    results <- rbind(results, data.frame(
      Iteration = i,
      NumGenes = num_genes,
      FeatureSelection = TRUE,
      
      Precision_Train = precision_train,
      Sensitivity_Train = sensitivity_train,
      Specificity_Train = specificity_train,
      
      Precision_Test = precision_test,
      Sensitivity_Test = sensitivity_test,
      Specificity_Test = specificity_test,
      
      Genes = paste(selected_genes, collapse = ",")
    )
    )
  }
}

# Sort by precision or another metric of your choice
top_results <- results[order(-results$Precision_Train), ]

top_results <- top_results[
  top_results$Precision_Train != 1 &
  top_results$Sensitivity_Train != 1 &
  top_results$Specificity_Train != 1 &
  top_results$Precision_Test != 1 &
  top_results$Sensitivity_Test != 1 &
  top_results$Specificity_Test != 1,
]

# Get the max values
max_precision <- max(top_results$Precision_Test, na.rm = TRUE)
max_sensitivity <- max(top_results$Sensitivity_Test, na.rm = TRUE)

# Filter to rows that match BOTH max precision and max sensitivity
best_results <- top_results[
  top_results$Precision_Test == max_precision |
  top_results$Sensitivity_Test == max_sensitivity,
]

# Considering the classification metrics Precision, Recall and Specificity and the trade-off between them. The iteration 12th have the best trade-off
# Precision: 0.7500000, Recall: 0.4285714, Specificity: 0.875, with a Total of 34 Genes as feature Selection. 
# As recommendation we could evaluate the normality of the features and perform transformation techniques to achieve a higher result.

# Choose the best row (you can change this logic if needed)
best_lda_row <- best_results[1, ]

# Prepare data for prediction again
selected_genes <- unlist(strsplit(as.character(best_lda_row$Genes), ","))

# Ensure selected genes exist in test set
lda_test_df <- data.frame(test_genes[, selected_genes], Class = test_labels)
lda_train_df <- data.frame(train_genes[, selected_genes], Class = train_labels)

# Refit LDA model
lda_model <- lda(Class ~ ., data = lda_train_df)
lda_pred <- predict(lda_model, newdata = lda_test_df)

# Confusion matrix
cm_lda <- confusionMatrix(
  data = as.factor(lda_pred$class),
  reference = as.factor(lda_test_df$Class),
  positive = "1"
)

# Extract metrics
accuracy_lda <- cm_lda$overall["Accuracy"]
sensitivity_lda <- cm_lda$byClass["Sensitivity"]
specificity_lda <- cm_lda$byClass["Specificity"]
ppv_lda <- cm_lda$byClass["Pos Pred Value"]
npv_lda <- cm_lda$byClass["Neg Pred Value"]

# AUC
roc_lda <- pROC::roc(response = as.factor(lda_test_df$Class),
               predictor = lda_pred$posterior[, "1"],
               levels = rev(levels(as.factor(lda_test_df$Class))))
auc_lda <- auc(roc_lda)

# Number of features
nfeats_lda <- length(selected_genes)


metrics_lda <- data.frame(
  Model = "LDA",
  Accuracy = as.numeric(accuracy_lda),
  Sensitivity = as.numeric(sensitivity_lda),
  Specificity = as.numeric(specificity_lda),
  PPV = as.numeric(ppv_lda),
  NPV = as.numeric(npv_lda),
  AUC = as.numeric(auc_lda),
  Nfeats = as.numeric(nfeats_lda)
)

```

# Model:Random Forest  
# Resample Techinque: K - Fold Cross Validation
```{r setup, include=FALSE}
set.seed(125) 

# Load Data
df <- load_gene_subset(group_number = 7, include_class = TRUE)

# shuffle data to minimize random effects
shuffled_df <-  df[sample(nrow(df)),]

# Separate predictors and class
genes_only <- shuffled_df[, setdiff(names(shuffled_df), "Class")]
all_genes <- colnames(genes_only)



# Create training 80% and test 20%, 
train <- createDataPartition(shuffled_df$Class, p = 0.8, list = FALSE, times = 1)
df_train <- shuffled_df[train,]
df_test <- shuffled_df[-train,]

# 1. Random Forest model using repeated 10-fold cross-validation
# With regression problems the default value is often  mtry = p3 and for classification mtry=âˆšp
num_features <- ncol(df_train) - 1
sqrt_mtry <- floor(sqrt(num_features))

tunegrid_rf <- expand.grid(
  mtry = c(max(1, sqrt_mtry - 2), sqrt_mtry, sqrt_mtry + 2)
)

# TrainControl must include classProbs = TRUE
ctrl <- trainControl(
  method = "cv",
  number = 10,
  classProbs = TRUE,
  savePredictions = "final",
  summaryFunction = twoClassSummary
)

# Target variable is a factor
df_train$Class <- as.factor(df_train$Class)
levels(df_train$Class) <- make.names(levels(df_train$Class))

# Train random forest model with tuneGrid and cross-validation
model_rf <- train(
  Class ~ .,
  data = df_train,
  method = "rf",
  trControl = ctrl,
  tuneGrid = expand.grid(mtry = floor(sqrt(ncol(df_train) - 1))),
  ntree = 100,
  metric = "ROC"
)

roc_obj <- pROC::roc(
  response = model_rf$pred$obs,
  predictor = model_rf$pred$X1,  # X1 is the class label "1" after make.names()
  levels = rev(levels(model_rf$pred$obs))
)

auc_value_rf <- auc(roc_obj)

# Get predicted classes and true labels
predictions <- model_rf$pred$pred
true_labels <- model_rf$pred$obs

# Confusion Matrix
cm <- confusionMatrix(predictions, true_labels)


# Metrics
accuracy_rf <- cm$overall["Accuracy"]
sensitivity_rf <- cm$byClass["Sensitivity"]
specificity_rf <- cm$byClass["Specificity"]
ppv_rf <- cm$byClass["Pos Pred Value"]
npv_rf <- cm$byClass["Neg Pred Value"]
nfeats_rf <- ncol(df_train) - 1 

# Normalize Nfeats for plotting (0 to 1 scale)
nfeats_norm <- nfeats / max(nfeats, 50)  # 50 is an arbitrary scale max

metrics_rf <- data.frame(
  Model = "RF",
  Accuracy = as.numeric(accuracy_rf),
  Sensitivity = as.numeric(sensitivity_rf),
  Specificity = as.numeric(specificity_rf),
  PPV = as.numeric(ppv_rf),
  NPV = as.numeric(npv_rf),
  AUC = as.numeric(auc_value_rf),
  Nfeats = as.numeric(nfeats_rf)
)

# Add rows for max and min for scaling in radar plot
metrics_plot <- rbind(
  rep(1, 7),  # max values for scaling
  rep(0, 7),  # min values
  metrics_df
)

radarchart(metrics_plot,
           axistype = 1,
           pcol = "blue",         # line color
           pfcol = rgb(0.2,0.5,0.5,0.3),  # fill color with transparency
           plwd = 2,              # line width
           cglcol = "grey",       # grid color
           cglty = 1,
           axislabcol = "black",
           caxislabels = seq(0, 1, 0.2),
           vlcex = 0.8)           # label size


title("rfGA")

library(fmsb)

# Combine
metrics_all <- rbind(metrics_rf, metrics_lda)

#Normalize all numeric columns to [0, 1] scale for fair plotting
normalize_df <- function(df) {
  metric_names <- colnames(df)[-1]  # exclude 'Model' column
  for (metric in metric_names) {
    min_val <- min(df[[metric]], na.rm = TRUE)
    max_val <- max(df[[metric]], na.rm = TRUE)
    
    if (min_val == max_val) {
      df[[metric]] <- 0.5  # or 1 if you want to emphasize
    } else {
      df[[metric]] <- (df[[metric]] - min_val) / (max_val - min_val)
    }
  }
  return(df)
}

metrics_all_scaled <- normalize_df(metrics_all)


metrics_rf_scaled <- metrics_rf
metrics_rf_scaled[,-1] <- as.data.frame(lapply(metrics_rf[,-1], normalize))

metrics_lda_scaled <- metrics_lda
metrics_lda_scaled[,-1] <- as.data.frame(lapply(metrics_lda[,-1], normalize))

# Function to prepare data for radar chart (fmsb package requires max and min rows)
prepare_radar_df <- function(df) {
  df <- df[,-1]  # remove Model column
  rbind(rep(1, ncol(df)), rep(0, ncol(df)), df)
}

rf_radar <- prepare_radar_df(metrics_rf_scaled)
lda_radar <- prepare_radar_df(metrics_lda_scaled)

# Plot settings
radar_colors <- c("blue", "red")

par(mfrow = c(1, 2))  # 1 row, 2 columns

# RF Radar Plot
radarchart(rf_radar,
           axistype = 1,
           pcol = radar_colors[1],
           pfcol = scales::alpha(radar_colors[1], 0.4),
           plwd = 2,
           title = "Random Forest")

# LDA Radar Plot
radarchart(lda_radar,
           axistype = 1,
           pcol = radar_colors[2],
           pfcol = scales::alpha(radar_colors[2], 0.4),
           plwd = 2,
           title = "LDA")


```