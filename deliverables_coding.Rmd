---
title: "Group Project"
output: pdf_document
date: "2025-06-03"
---

# FUNCTIONS
```{r setup, include=FALSE}

load_gene_subset <- function(group_number,
                             expression_file = "gene-expression-invasive-vs-noninvasive-cancer.csv",
                             subset_file = "teamsubsets.csv",
                             include_class = TRUE) {
  library(dplyr)
  library(caret)
  library(glmnet)
  library(ggplot2)
  library(reshape2)
  library(pheatmap)
  library(MASS)

  # Load main dataset
  dataframe <- read.csv(file = expression_file)

  # Load subset file
  df_subsets <- read.csv(file = subset_file, sep = ' ')

  # Filter by group number
  reg_id <- df_subsets[group_number, ]

  # Extract gene indices (remove the ID column)
  subsets <- as.numeric(reg_id[-1])

  # Filter gene expression dataframe using selected gene indices
  df <- dataframe[, subsets]

  # Optionally, add class and cancer type
  if (include_class) {
    df$Class <- dataframe$Class
  }

  return(df)
}

# Initialize empty results data frame
results_table <- data.frame(
  Model = character(),
  Hyperparameter = character(),
  Precision = numeric(),
  Sensitivity = numeric(),
  Specificity = numeric(),
  stringsAsFactors = FALSE
)

# Define the function
add_results <- function(model, hyperparameter, precision, sensitivity, specificity) {
  new_row <- data.frame(
    Model = model,
    Hyperparameter = hyperparameter,
    Precision = precision,
    Sensitivity = sensitivity,
    Specificity = specificity,
    stringsAsFactors = FALSE
  )
  
  # Use global assignment to update results_table
  assign("results_table", rbind(results_table, new_row), envir = .GlobalEnv)
}

# Fisher Criterion Function (for multiple features)
fisher_criterion <- function(X, y) {
  classes <- unique(y)
  if (length(classes) != 2) stop("Only works for binary classification.")
  
  X1 <- X[y == classes[1], , drop = FALSE]
  X2 <- X[y == classes[2], , drop = FALSE]
  
  mean1 <- colMeans(X1)
  mean2 <- colMeans(X2)
  
  Sw1 <- cov(X1)
  Sw2 <- cov(X2)
  Sw <- Sw1 + Sw2
  
  n1 <- nrow(X1)
  n2 <- nrow(X2)
  mean_diff <- matrix(mean1 - mean2, ncol = 1)
  Sb <- (n1 * n2) / (n1 + n2) * (mean_diff %*% t(mean_diff))
  
  J <- sum(diag(Sb)) / sum(diag(Sw))
  return(J)
}

# Greedy Feature Selection
greedy_fisher_selection_minmax <- function(X, y, d, min_gain_threshold = 0.01) {
  remaining_features <- colnames(X)
  selected_features <- c()
  selected_scores <- c()
  
  current_score <- 0
  
  for (k in 1:d) {
    best_gain <- -Inf
    best_feature <- NULL
    best_score <- NULL
    
    for (feature in setdiff(remaining_features, selected_features)) {
      candidate_features <- c(selected_features, feature)
      new_score <- fisher_criterion(X[, candidate_features, drop = FALSE], y)
      gain <- new_score - current_score
      
      if (gain > best_gain) {
        best_gain <- gain
        best_feature <- feature
        best_score <- new_score
      }
    }
    
    # Stop if marginal gain is too small
    if (best_gain < min_gain_threshold) {
      message(sprintf("Stopping early at step %d — ΔJ = %.4f < threshold %.4f", k, best_gain, min_gain_threshold))
      break
    }
    
    selected_features <- c(selected_features, best_feature)
    selected_scores <- c(selected_scores, best_score)
    current_score <- best_score
    
    message(sprintf("Selected %s (step %d) | Gain: %.4f | Fisher Score: %.4f", best_feature, k, best_gain, best_score))
  }
  
  return(list(features = selected_features, scores = selected_scores))
}

# Advanced Greedy Feature Selection using Fisher Criterion + Correlation Penalty
# Combines Fisher Score Gain and Redundancy Penalty

greedy_fisher_selection_with_penalty <- function(X, y, d = 50, lambda = 0.5, min_gain_threshold = 0.01) {
  selected_features <- c()
  selected_scores <- c()
  feature_names <- colnames(X)
  remaining_features <- feature_names

  # Standardize X for correlation computation
  X_scaled <- scale(X)

  for (k in 1:d) {
    best_score <- -Inf
    best_feature <- NULL
    best_gain <- NULL

    for (candidate in setdiff(remaining_features, selected_features)) {
      # Evaluate Fisher score with candidate + existing selected
      candidate_features <- c(selected_features, candidate)
      fisher_score_new <- fisher_criterion(X_scaled[, candidate_features, drop = FALSE], y)

      # Compute previous Fisher score
      if (length(selected_features) == 0) {
        fisher_score_prev <- 0
      } else {
        fisher_score_prev <- fisher_criterion(X_scaled[, selected_features, drop = FALSE], y)
      }

      delta_j <- fisher_score_new - fisher_score_prev

      # Compute redundancy penalty
      if (length(selected_features) > 0) {
        correlations <- sapply(selected_features, function(sf) {
          abs(cor(X_scaled[, candidate], X_scaled[, sf], use = "pairwise.complete.obs"))
        })
        redundancy_penalty <- max(correlations)
      } else {
        redundancy_penalty <- 0
      }

      # Final score: Fisher gain - lambda * redundancy
      final_score <- delta_j - lambda * redundancy_penalty

      if (final_score > best_score) {
        best_score <- final_score
        best_feature <- candidate
        best_gain <- delta_j
      }
    }

    # Stop if gain is too low
    if (best_gain < min_gain_threshold) {
      message(sprintf("Stopping early at step %d — ΔJ = %.4f < threshold %.4f", 
                      k, best_gain, min_gain_threshold))
      break
    }

    selected_features <- c(selected_features, best_feature)
    selected_scores <- c(selected_scores, best_score)
    message(sprintf("Selected %s (step %d) | Gain: %.4f | Score: %.4f", 
                    best_feature, k, best_gain, best_score))
  }

  return(list(features = selected_features, scores = selected_scores))
}


```

# FEATURE SELECTION
# To tackle the first point we implmented different techniques to approach different point of views

# Contribution Performance
# LDA (weights) - FEATURE SELECTION 
```{r setup, include=FALSE}

set.seed(123) 
results <- data.frame(Iteration = integer(),
                      NumGenes = integer(),
                      Precision = double(),
                      Sensitivity = double(),
                      Specificity = double(),
                      Genes = character(),
                      stringsAsFactors = FALSE)

# LDA Dimensionality Reduction and Model  -- VERSION 1
# The approach using LDA for dimensionality reduction was: 
# 1. Identify contribution performance using LDA to get features weights
# 2. Perform correlation matrix
# 3. Compute LDA weights


# Load Data
df <- load_gene_subset(group_number = 7, include_class = TRUE)

# Extract Predictors
gene_data <- df[, setdiff(names(df), "Class")]

#To avoid collinearity (redundant variables) removed correlated features
# Compute correlation matrix
cor_matrix <- cor(df, method = "pearson", use = "pairwise.complete.obs")
cor_df <- as.data.frame(as.table(cor_matrix))

# Remove self-correlations
cor_df <- cor_df[cor_df$Var1 != cor_df$Var2, ]

#Remove duplicate pairs (since correlation matrix is symmetric)
cor_df <- cor_df[!duplicated(t(apply(cor_df[, 1:2], 1, sort))), ]

# Filter for highly correlated pairs (e.g., |r| > 0.9)
high_cor_df <- cor_df[abs(cor_df$Freq) > 0.8, ]

# Strategy: Keep only one variable from each correlated pair
genes_to_remove <- unique(high_cor_df$Var2)
length(genes_to_remove)

# Create reduced dataframe by removing redundant genes
df_reduced <- df[, !(colnames(df) %in% genes_to_remove)]

# Separate predictors and class
genes_only <- df_reduced[, setdiff(names(df_reduced), "Class")]
labels <- df_reduced$Class
all_genes <- colnames(genes_only)

# Create stratified train/test split
train_index <- createDataPartition(labels, p = 0.8, list = FALSE)
train_genes <- genes_only[train_index, ]
test_genes <- genes_only[-train_index, ]
train_labels <- labels[train_index]
test_labels <- labels[-train_index]

# Column-wise means
train_means <- colMeans(train_genes)
test_means <- colMeans(test_genes)

# Column-wise variances
train_vars <- apply(train_genes, 2, var)
test_vars  <- apply(test_genes, 2, var)

# Global average mean and variance to avoid Data Leakage
overall_train_mean <- mean(train_means)
overall_test_mean <- mean(test_means)

overall_train_var <- mean(train_vars)
overall_test_var <- mean(test_vars)

cat("Train Mean:", overall_train_mean, "\n")
cat("Test Mean:", overall_test_mean, "\n")
cat("Train Variance:", overall_train_var, "\n")
cat("Test Variance:", overall_test_var, "\n")


# ----- Save Training and Test Data -----
# Combine gene expression and class label
#train_df <- data.frame(train_genes, Class = train_labels)
#test_df <- data.frame(test_genes, Class = test_labels)

# Save to CSV files
#write.csv(train_df, "train_data.csv", row.names = FALSE)
#write.csv(test_df, "test_data.csv", row.names = FALSE)
# --------------------------------------

# === Random Search for Best Gene Subset ===
n_iter <- 30  # number of random samples
gene_range <- 10:60  # range of gene subset sizes to try
results <- data.frame()


for (i in 1:n_iter) {
  num_genes <- sample(gene_range, 1)
  selected_genes <- sample(all_genes, num_genes)
  
  # Prepare data
  df_filtered <- data.frame(genes_only[, selected_genes], Class = labels)
  
  # Prepare training and test sets with selected genes
  train_df <- data.frame(train_genes[, selected_genes], Class = train_labels)
  test_df <- data.frame(test_genes[, selected_genes], Class = test_labels)
  
  
  # Fit LDA model
  lda_model <- tryCatch({
    lda(Class ~ ., data = train_df)
  }, error = function(e) return(NULL))
  
  if (!is.null(lda_model)) {
    # Predict on test set
    test_pred <- predict(lda_model, newdata = test_df)
    cm_test <- confusionMatrix(as.factor(test_pred$class), as.factor(test_df$Class), positive = "1")
    
    # Predict on training set
    train_pred <- predict(lda_model, newdata = train_df)
    cm_train <- confusionMatrix(as.factor(train_pred$class), as.factor(train_df$Class), positive = "1")
    
    # Extract metrics
    precision_test <- cm_test$byClass["Pos Pred Value"]
    sensitivity_test <- cm_test$byClass["Sensitivity"]
    specificity_test <- cm_test$byClass["Specificity"]
    
    precision_train <- cm_train$byClass["Pos Pred Value"]
    sensitivity_train <- cm_train$byClass["Sensitivity"]
    specificity_train <- cm_train$byClass["Specificity"]
    
    # Append to results
    results <- rbind(results, data.frame(
      Iteration = i,
      NumGenes = num_genes,
      
      Precision_Train = precision_train,
      Sensitivity_Train = sensitivity_train,
      Specificity_Train = specificity_train,
      
      Precision_Test = precision_test,
      Sensitivity_Test = sensitivity_test,
      Specificity_Test = specificity_test,
      
      Genes = paste(selected_genes, collapse = ",")
    ))
  }
}

# Sort by precision or another metric of your choice

top_results <- results[order(-results$Precision_Train), ]

top_results <- top_results[
  top_results$NumGenes >= 35 &
  top_results$Precision_Train != 1 &
  top_results$Sensitivity_Train != 1 &
  top_results$Specificity_Train != 1,
]

# Show top-performing gene sets
print(head(top_results, 5))



```
